{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07f83c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (4.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: playwright in /home/os1r1s/.local/lib/python3.10/site-packages (1.57.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in /home/os1r1s/.local/lib/python3.10/site-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /home/os1r1s/.local/lib/python3.10/site-packages (from playwright) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /home/os1r1s/.local/lib/python3.10/site-packages (from pyee<14,>=13->playwright) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: playwright-stealth in /home/os1r1s/.local/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (2.25.1)\n",
      "Requirement already satisfied: playwright<2.0.0,>=1.0.0 in /home/os1r1s/.local/lib/python3.10/site-packages (from playwright-stealth) (1.57.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in /home/os1r1s/.local/lib/python3.10/site-packages (from playwright<2.0.0,>=1.0.0->playwright-stealth) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /home/os1r1s/.local/lib/python3.10/site-packages (from playwright<2.0.0,>=1.0.0->playwright-stealth) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /home/os1r1s/.local/lib/python3.10/site-packages (from pyee<14,>=13->playwright<2.0.0,>=1.0.0->playwright-stealth) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4\n",
    "%pip install playwright\n",
    "%pip install playwright-stealth requests\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e696723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Sitemap Index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 sub-sitemaps. Checking the latest ones...\n",
      "Found 0 attractions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import io\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# The Sitemap Index you provided (Attractions)\n",
    "SITEMAP_INDEX = \"https://www.tripadvisor.com/sitemap/att/en_US/sitemap_en_US_attractions_index.xml\"\n",
    "\n",
    "# Your target cities\n",
    "TARGET_GEO_IDS = [\"g187323\"] # Berlin\n",
    "\n",
    "def get_urls_from_sitemap():\n",
    "    print(\"Fetching Sitemap Index...\")\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "    r = requests.get(SITEMAP_INDEX, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'xml')\n",
    "    \n",
    "    sub_sitemaps = [loc.text for loc in soup.find_all('loc')]\n",
    "    \n",
    "    found_urls = []\n",
    "\n",
    "    # We iterate through the sub-sitemaps (There are many, maybe limit this for testing)\n",
    "    print(f\"Found {len(sub_sitemaps)} sub-sitemaps. Checking the latest ones...\")\n",
    "    \n",
    "    # In a real run, you might loop through all. \n",
    "    # For now, let's look at the last 3 (most recent) to see if we find matches.\n",
    "    for sub_map_url in sub_sitemaps[-3:]: \n",
    "        print(f\"Checking: {sub_map_url}\")\n",
    "        try:\n",
    "            r_sub = requests.get(sub_map_url, headers=headers)\n",
    "            \n",
    "            # Decompress GZIP\n",
    "            with gzip.open(io.BytesIO(r_sub.content), 'rb') as f:\n",
    "                xml_content = f.read()\n",
    "                \n",
    "            soup_sub = BeautifulSoup(xml_content, 'xml')\n",
    "            urls = [loc.text for loc in soup_sub.find_all('loc')]\n",
    "            \n",
    "            # Filter for our GeoID\n",
    "            for url in urls:\n",
    "                for geo in TARGET_GEO_IDS:\n",
    "                    # Pattern matching for TripAdvisor Attraction URLs containing the GeoID\n",
    "                    if f\"-{geo}-\" in url and \"Activities-\" in url:\n",
    "                        found_urls.append(url)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing map: {e}\")\n",
    "\n",
    "    return found_urls\n",
    "\n",
    "# Run this once to generate your \"ToDo\" list\n",
    "target_urls = get_urls_from_sitemap()\n",
    "print(f\"Found {len(target_urls)} attractions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6abf0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Berlin ---\n",
      "Navigating to offset 0...\n",
      "!!! BLOCKED BY TRIPADVISOR !!!\n",
      "Please check the browser window. Waiting 60 seconds...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Run this cell\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scrape_with_stealth(cities)\n",
      "Cell \u001b[0;32mIn[27], line 68\u001b[0m, in \u001b[0;36mscrape_with_stealth\u001b[0;34m(cities)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Pause to let you solve it manually if possible\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the browser window. Waiting 60 seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Check for \"Challenge\" (Cloudflare/Anti-bot)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChallenge\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mtitle():\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:605\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    601\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[1;32m    602\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[1;32m    603\u001b[0m                     future, result)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import os\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright_stealth import Stealth  # Import the class, not the function\n",
    "\n",
    "# Create debug folder\n",
    "if not os.path.exists(\"debug_screens\"):\n",
    "    os.makedirs(\"debug_screens\")\n",
    "if not os.path.exists(\"html_dumps\"):\n",
    "    os.makedirs(\"html_dumps\")\n",
    "\n",
    "cities = [\n",
    "    {\"name\": \"Berlin\", \"geo\": \"g187323\"},\n",
    "]\n",
    "\n",
    "async def scrape_with_stealth(cities):\n",
    "    # --- CHANGED: Use Stealth().use_async() wrapper ---\n",
    "    async with Stealth().use_async(async_playwright()) as p:\n",
    "        \n",
    "        # 1. Launch Browser\n",
    "        # We must keep headless=False because TripAdvisor is extremely sensitive to headless browsers.\n",
    "        browser = await p.chromium.launch(\n",
    "            headless=False, \n",
    "            args=[\n",
    "                \"--disable-blink-features=AutomationControlled\", \n",
    "                \"--start-maximized\"\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 2. Configure Context (User Agent & Viewport are critical)\n",
    "        context = await browser.new_context(\n",
    "            viewport={'width': 1920, 'height': 1080},\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "            locale=\"en-US\"\n",
    "        )\n",
    "        \n",
    "        # Note: In this new API, stealth is applied automatically to contexts/pages created here\n",
    "        page = await context.new_page()\n",
    "\n",
    "        for city in cities:\n",
    "            print(f\"--- Processing {city['name']} ---\")\n",
    "            \n",
    "            # Start loop: offset 0, 30, 60...\n",
    "            for offset in range(0, 91, 30): \n",
    "                \n",
    "                if offset == 0:\n",
    "                    url = f\"https://www.tripadvisor.com/Attractions-{city['geo']}-Activities-{city['name']}.html\"\n",
    "                else:\n",
    "                    url = f\"https://www.tripadvisor.com/Attractions-{city['geo']}-Activities-oa{offset}-{city['name']}.html\"\n",
    "\n",
    "                print(f\"Navigating to offset {offset}...\")\n",
    "                \n",
    "                try:\n",
    "                    # Random \"human\" pause before clicking/navigating\n",
    "                    await asyncio.sleep(random.uniform(2, 5))\n",
    "                    \n",
    "                    response = await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "                    \n",
    "                    # 4. CHECK IF BLOCKED\n",
    "                    page_content = await page.content()\n",
    "                    \n",
    "                    if \"Access Denied\" in page_content or (response and response.status == 403):\n",
    "                        print(\"!!! BLOCKED BY TRIPADVISOR !!!\")\n",
    "                        await page.screenshot(path=\"debug_screens/blocked.png\")\n",
    "                        # Pause to let you solve it manually if possible\n",
    "                        print(\"Please check the browser window. Waiting 60 seconds...\")\n",
    "                        await asyncio.sleep(60)\n",
    "                    \n",
    "                    # Check for \"Challenge\" (Cloudflare/Anti-bot)\n",
    "                    if \"Challenge\" in await page.title():\n",
    "                        print(\"!!! CAPTCHA DETECTED !!! - Please solve it in the browser window.\")\n",
    "                        await asyncio.sleep(30) \n",
    "\n",
    "                    # 5. Human-like Scrolling\n",
    "                    # TripAdvisor relies heavily on lazy loading. If you don't scroll, you won't get all 30 items.\n",
    "                    try:\n",
    "                        await page.wait_for_selector('footer', state='attached', timeout=10000)\n",
    "                    except:\n",
    "                        pass # If footer not found, just scroll anyway\n",
    "                        \n",
    "                    for _ in range(3):\n",
    "                        await page.mouse.wheel(0, 600)\n",
    "                        await asyncio.sleep(random.uniform(0.5, 1.5))\n",
    "                    \n",
    "                    # 6. Save Data\n",
    "                    filename = f\"html_dumps/{city['name']}_oa{offset}.html\"\n",
    "                    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                        f.write(await page.content())\n",
    "                    \n",
    "                    print(f\" -> Saved {filename}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error on {url}: {e}\")\n",
    "                    await page.screenshot(path=f\"debug_screens/error_{offset}.png\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "# Run this cell\n",
    "await scrape_with_stealth(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async with async_playwright() as p:\n",
    "    browser = await p.chromium.launch(headless=False, args=[\"--disable-blink-features=AutomationControlled\"])\n",
    "    context = await browser.new_context()\n",
    "    await context.add_init_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    page = await context.new_page()\n",
    "    await page.goto(\"https://bot.sannysoft.com/\")\n",
    "    await asyncio.sleep(5)\n",
    "    # Check the page visually to see if 'WebDriver' is green (false)\n",
    "    await browser.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
